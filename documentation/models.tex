%!TEX root = main.tex
\section{Models}
\label{sec:models}
\subsection{Considered models}
\subsubsection{Logistic Regression}
    ~\\
    For our dynamic pricing strategy we first evaluated a binary Logistic Regression model. Generally, they are used to estimate a categorical dependent variable based on a logistic function. In our case, we estimate the probability of a binary response (sold or not). Hence, we use a binary logistic model. Otherwise, if there more than two possible outcome classes, it is called multinomial Logistic Regression.
    Our approach creates a coordinate system, where each point represents a market situation based on the given features. Now, it models the expected value by a linear function in such a way, that the squared error is as small as possible. The resulting function represents the coherence between the features ($X_1, X_2, ..., X_m$) and the expected dependent variable $E(Y)$ as follows:

    \begin{equation}
    \label{eq.logreg.lin}
    E(Y) = \beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\cdots +\beta _{m}x_{m},
    \end{equation}

    where $\beta _m$ are the coefficients to be determined. Until that point, there is no difference between Linear and Logistic Regression. Since we want to calculate a sale probability for a given market situation, we have to apply a logistic function in addition to our existing one. Therefore, we use

    \begin{equation}
    \label{eq.logreg.log}
    f(p)=log\left(\frac{p}{1-p}\right).
    \end{equation}

    The combination of formula~\ref{eq.logreg.lin} and~\ref{eq.logreg.log} results in

    \begin{equation}
    \label{eq.logreg.loglin}
    E(Y) = log\left(\frac{p}{1-p}\right) = \beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\cdots +\beta _{m}x_{m},
    \end{equation}

    which is no longer a linear function, but a logistic one. Putting left and right side together, the resulting formula

    \begin{equation}
    \label{eq.logreg.logsolved}
    E(Y) = p = \frac{e^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\cdots +\beta _{m}x_{m}}}{1+e^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\cdots +\beta _{m}x_{m}}},
    \end{equation}

    calculates the probability of selling or not selling a product at a given market situation.

\subsubsection{MLP}
    ~\\
    Multilayer Perceptron (MPL) is ...
\subsubsection{Random Forest}
    ~\\
    Random Forests belong to the group of ensemble learning methods consisting of multiple decision trees. It can be used for both, classification and regression. Each tree grows randomly during the learning process by dividing the training set into subsets. A decision tree consists of one root node and multiple sub and leaf nodes. Except the leaves, every node branch out into two subnodes. A node has the task to make exactly one decision based on a single variable. Leaf nodes have their own values. When predicting with a Random Forest regressor the input data will be processed by every decision tree in parallel. That is a huge performance advance compared to support vector machines or neural nets for example. The average of the results of each tree is the final prediction.
    In our implementation, we use 75 estimators. A lower number of trees results in a worse performance, a higher number increases the learning and prediction time significantly.
\subsubsection{Least Squares}
	~\\
	Additionally to the three different machine learning approaches, we also evaluated another technique: Least Squares. This method creates a function which approximates as good as possible a set of given measurement values by modeling a function having least squares to the measured values (in our case we have chosen a linear function). Adapted to our use case, it creates a function from the different prices, a product has been sold for. By using this function, we can get the price for which the probability of a sale is the highest. Furthermore, a trade-of between highest probability and highest profit can be presumed.
\subsection{Evaluation}
\label{sec:model_eval}
	~\\
	After a basic implementation of each technique, we evaluated all. Right from the beginning, the least squares approach did not prove to be competitive to the machine learning approaches. Although all approaches were on a similar level at the beginning, the machine learning approaches became better over time, while the least squares approach remained at this level.

	So, three techniques have been left over. Because the differences were relatively small, we gave us some more time to evaluate these techniques and started to optimize some relevant parameters, such as features, model parameters, etc.. At the same time, we also thought about further options to optimize our merchant independently from the concrete machine learning approach. This led to some additional functionalities of our merchant (see chapter \ref{sec:add_func}).

    Finally we decided to use Random Forest as basis for our merchant. It provided the best all-round performance and fits best our expectations. Detailed results of our evaluation can be found in chapter \ref{sec:performance}.




% http://logisticregressionanalysis.com/86-what-is-logistic-regression/
% https://en.wikipedia.org/wiki/Logistic_regression

%https://www.statistik.tu-dortmund.de/fileadmin/user_upload/Lehrstuehle/Genetik/BS1213/Biostatistik_mit_R_Knorre.pdf
